# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'pATRMainWindow.ui'
#
# Created by: PyQt5 UI code generator 5.15.4
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.
# from camUI import Ui_Dialog
from torch.utils.data import TensorDataset,DataLoader
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.autograd import Variable
from LeftTabStacked import LeftTabWidget
from PyQt5.QtCore import Qt, QTimer
from PyQt5.QtGui import QIcon
from lib.SlidingStackedWidget import SlidingStackedWidget
import os
from PyQt5.QtWidgets import QWidget, QLabel,QFileDialog
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtGui import *
from PyQt5.QtCore import *
from PyQt5.QtWidgets import *
import pyttsx3
from PyQt5.QtCore import pyqtSlot, QDir
from CNN_image_adv_test import Thread_CNN_image_adv_test
from PyQt5.QtWidgets import QVBoxLayout, QHBoxLayout, QWidget , QSlider, QLabel, QStackedLayout, QMessageBox
from setting_CNN import Ui_setting_CNN
from lib import VideoPlayerControlBar,ImageView,camUI
from CNN_image_test import Thread_CNN_image_test
from openpyxl import load_workbook
import scipy.misc as im
import model
import joblib
import numpy as np
import data
import random
from setting_dataset import Ui_setting_dataset
import torch
import torch.nn.functional as F
import time
import cv2
from skimage import transform
from torchvision import datasets, models, transforms

device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
from video_maker import video_maker
class CNN_setting_widget(Ui_setting_CNN,QWidget):
    def __init__(self):
        super(CNN_setting_widget, self).__init__()
        self.setupUi(self)
        self.caolianjie()


def write_excel_xlsx(path, value,tiao =  0):
    index = len(value)
    # 打开excel
    xl = load_workbook(path)
    # 获取所有sheet页名字
    xl_sheet_names = xl.get_sheet_names()
    # 定位到相应sheet页,[0]为sheet页索引
    xl_sheet = xl.get_sheet_by_name(xl_sheet_names[0])
    # 获取行列数
    row = xl_sheet.max_row
    if row == 0:
        ge = -2
    else:
        ge = 2
    # sheet.title = sheet_name
    for i in range(0, index):
        xl_sheet.cell(row=i + ge +row +tiao, column=1, value=str(value[i]))
    xl.save(path)
    # print("xlsx格式表格写入数据成功！")

class Thread_conventional_pichuli_test(QThread):
    update_imagetest = pyqtSignal(dict)

    def __init__(self,dataset_test_address,conventional_classifier):
        super().__init__()
        self.dataset_test_address = dataset_test_address.replace('\\', '/')
        self.image_name = os.listdir(dataset_test_address)
        self.conventional_img_classifier = conventional_classifier
        self.height, self.width, self.crop_size=600, 600,600
    def run(self):

        a=[]
        for i in range(len(self.image_name)):
            a.append(i)
            random.shuffle(a)
        for i in range(len(self.image_name)):
            test_result = self.image_test(a[i],i)
            self.update_imagetest.emit(test_result)
            # time.sleep(0.5)

    def image_test(self,t,i,dataset = 'uc'):
        # ##随机打乱
        # if dataset == 'uc':
        #     image_shape = (224, 224)
        #     channel = 3
        #     sub_dir = os.listdir('Data\\UCMerced_LandUse\\val')
        #     mean0, mean1, mean2, std0, std1, std2 = float(0.485), float(0.456), float(0.406), float(0.229), float(
        #         0.224), float(0.225)
        # elif dataset == 'mstar':
        #     image_shape = (128, 128)
        #     channel = 1
        #     sub_dir = os.listdir('Data\\MSTAR_zxy\\val')
        #     mean0 = 0.184
        #     std0 = 0.119
        # img = transform.resize(frame, image_shape)

        img = im.imresize(im.imread(self.dataset_test_address+'/'+self.image_name[t]), [self.height, self.width])
        img = img[(self.height - self.crop_size) // 2: self.height - (self.height - self.crop_size) // 2, \
              (self.width - self.crop_size) // 2: self.width - (self.width - self.crop_size) // 2]

        if img.shape[0] != 3:
            # plt.imshow(img, cmap='gray')

            img = img[np.newaxis,]
        # else:
        #     plt.imshow(img)
        # plt.show()
        img = np.reshape(img, [img.shape[0], img.shape[1] * img.shape[2] * img.shape[3]])
        img = img/255
        img = data.mean_wise(img)
        pca_trans = joblib.load('pths/pca_trans.pth')
        X_test = pca_trans.transform(img)
        classifier = joblib.load('pths/{}.pth'.format(self.conventional_img_classifier))
        test_result = model.test(X_test,classifier)

        # print(dataset)

        sub_dir = ['J0','J1','J2','J3','J4','J5','J6','J7','J8','J9']
        # print(sub_dir)
        if i == 0 :
            imagetest_out_log=['传统方法图像批处理识别： ','\n','分类结果： '+sub_dir[int(test_result)]]
            imagetest_out_log2 = ['传统方法图像批处理识别： ',  '图片地址： ' + self.dataset_test_address+'/'+self.image_name[t],
                                 '分类结果： ' + sub_dir[int(test_result)]]

            write_excel_xlsx('log.xlsx', imagetest_out_log2)
            out1 = ''.join(imagetest_out_log)
        else:
            imagetest_out_log = [ '分类结果： ' + sub_dir[int(test_result)]]
            imagetest_out_log2 = ['图片地址： ' + self.dataset_test_address+'/'+self.image_name[t],
                                  '分类结果： ' + sub_dir[int(test_result)]]

            write_excel_xlsx('log.xlsx', imagetest_out_log2,tiao=-1)
        out1 = ''.join(imagetest_out_log)
        a = random.randint(0, 360)
        b = random.randint(0, 360) / 10

        c = random.randint(40, 80) * 50
        d = random.randint(10, 30)
        e = random.randint(10, 20)
        f = random.randint(8, 20) * 10

        out = {0:out1,1:self.dataset_test_address+'/'+self.image_name[t],2:sub_dir[int(test_result)],3:[a,b,c,d,e,f]}

        return (out)
class Thread_CNN_imagepichuli_test(QThread):
    update_imagetest = pyqtSignal(dict)

    def __init__(self, dataset_test_address, model_address):
        super().__init__()

        self.dataset_test_address, self.model_address, self.channel_num, self.resize, self.centercrop, self.mean0, self.mean1, self.mean2, self.std0, self.std1, self.std2=\
            dataset_test_address.replace('\\', '/'), model_address.replace('\\', '/'), '3', 600, 600, float(0.485), float(0.456), float(0.406), float(0.229), float(0.224), float(0.225)
        self.image_names = os.listdir(dataset_test_address)

        self.cond = QWaitCondition()
        self.mutex = QMutex()
        self._ispause = False
    def pause(self):
        print('暂停')
        self._ispause = True
    def resume(self):
        print('开启')
        self._ispause = False
    def run(self):
        self.mutex.lock()
        if self._ispause: self.cond.wait(self.mutex)
        a=[]
        for i in range(len(self.image_names)):
            a.append(i)
            random.shuffle(a)
        for i in range(len(self.image_names)):
            test_result = self.image_test(a[i],i)
            self.update_imagetest.emit(test_result)
            # time.sleep(0.5)
        self.mutex.unlock()
    def image_test(self,t,i):
        img = im.imresize(im.imread(self.dataset_test_address+'/'+self.image_names[t]), [self.resize, self.resize])

        img = img / 255
        if self.channel_num == '3':
            img = np.transpose(img, (2, 0, 1))
            img[0] = (img[0]-self.mean0 )/self.std0
            img[1] = (img[1]-self.mean1 )/self.std1
            img[2] = (img[2]-self.mean2 )/self.std2

        elif self.channel_num == '1':
            img = (img-self.mean0)/self.std0
            img = img[np.newaxis,:,:]
        img = img[:,(self.resize-self.centercrop)//2:(self.resize+self.centercrop)//2,(self.resize-self.centercrop)//2:(self.resize+self.centercrop)//2]

        img = img[np.newaxis,:,:,:]
        img = np.float32(img)

        img = torch.from_numpy(img)
        img = img.to(device)
        model = torch.load(self.model_address)
        model = model.to(device)

        result = model(img)[0]

        result = F.softmax(result)
        sub_dir = ['J0', 'J1', 'J2', 'J3', 'J4', 'J5', 'J6', 'J7', 'J8', 'J9']

        if i == 0:
            self.out = ['CNN图像批处理识别：', '\n', '图像路径：', self.dataset_test_address+'\\'+self.image_names[t], '\n', '参数设置： ']
            self.out.extend(['\n','识别结果为：',sub_dir[int(torch.max(result,0).indices)]])
        else:
            self.out = [ '图像路径：', self.dataset_test_address+'\\'+self.image_names[t], '\n', '参数设置： ']
            self.out.extend(['\n','识别结果为：',sub_dir[int(torch.max(result,0).indices)]])
        pred = sub_dir[int(torch.max(result,0).indices)]

        out = [str(i) for i in self.out]
        out = ''.join(out)
        CNN_image_test_out_log = out.split('\n')
        write_excel_xlsx('log.xlsx', CNN_image_test_out_log)
        result = result.cpu().detach().numpy()
        a = random.randint(0, 360)
        b = random.randint(0, 360) / 10

        c = random.randint(40, 80) * 50
        d = random.randint(10, 30)
        e = random.randint(10, 20)
        f = random.randint(8, 20) * 10
        return ({0:self.dataset_test_address+'\\'+self.image_names[t],1:out,2:result,3:sub_dir,4:pred,5:[a,b,c,d,e,f]})



class Ui_HQ16pATR(object):
    def setupUi(self, HQ16pATR):
        self.mw = HQ16pATR

        HQ16pATR.setObjectName("HQ16pATR")
        HQ16pATR.setWindowModality(QtCore.Qt.NonModal)
        HQ16pATR.resize(1000, 682)
        HQ16pATR.setAutoFillBackground(True)
        HQ16pATR.setAnimated(True)
        self.centralwidget = QtWidgets.QWidget(HQ16pATR)
        self.centralwidget.setObjectName("centralwidget")

      #  grid=QGridLayout(self.centralwidget)

        self.groupBoxIMG = QtWidgets.QGroupBox(self.centralwidget)

        #grid.addWidget(self.groupBoxIMG,1,1)

        L1 = QtWidgets.QVBoxLayout(self.groupBoxIMG)
        L1.setContentsMargins(0, 0, 0, 0)

        #grid.addLayout(L1,1,1)

        self.groupBoxIMG.setEnabled(True)
        self.groupBoxIMG.setGeometry(QtCore.QRect(10, 10, 441, 341))
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(11)
        font.setBold(False)
        font.setWeight(50)
        self.groupBoxIMG.setFont(font)
        self.groupBoxIMG.setObjectName("groupBoxIMG")

        self.IMGDisplay = QtWidgets.QLabel(self.groupBoxIMG)
        self.IMGDisplay.setGeometry(QtCore.QRect(10, 20, 421, 280))
        self.IMGDisplay.setScaledContents(True)
        self.IMGDisplay.setAutoFillBackground(True)
        self.IMGDisplay.setFrameShape(QtWidgets.QFrame.Box)
        self.IMGDisplay.setFrameShadow(QtWidgets.QFrame.Raised)
        self.IMGDisplay.setLineWidth(3)
        self.IMGDisplay.setMidLineWidth(0)
        self.IMGDisplay.setText("")
        self.IMGDisplay.setObjectName("IMGDisplay")
        L1.addWidget(self.IMGDisplay)



        # Add VideoPlayerControlBar widget
        self.videoPlayerControlBar = VideoPlayerControlBar.VideoPlayerControlBar()
        self.videoPlayerControlBar.setupUi(self)
       # self.videoPlayerControlBar.show()
        self.videoPlayerControlBar.setGeometry(QtCore.QRect(10, 301, 421, 340))
        L1.addWidget(self.videoPlayerControlBar)


        self.groupBoxSET = QtWidgets.QGroupBox(self.centralwidget)
       # grid.addWidget(self.groupBoxSET, 1, 2)
        L2=QHBoxLayout(self.groupBoxSET)
        L2_L1=QVBoxLayout()
        L2_L2=QVBoxLayout()
        L2_L3=QHBoxLayout()

        self.groupBoxSET.setGeometry(QtCore.QRect(480, 10, 521, 241))
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(10)
        self.groupBoxSET.setFont(font)
        self.groupBoxSET.setObjectName("groupBoxSET")



        self.pushButtonIMG = QtWidgets.QPushButton(self.groupBoxSET)
       # self.pushButtonIMG.setGeometry(QtCore.QRect(10, 40, 121, 31))
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(10)
        self.pushButtonIMG.setFont(font)
        self.pushButtonIMG.setAutoFillBackground(True)
        self.pushButtonIMG.setObjectName("pushButtonIMG")
        self.pushButtonSEQ = QtWidgets.QPushButton(self.groupBoxSET)
       # self.pushButtonSEQ.setGeometry(QtCore.QRect(10, 90, 121, 31))
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(10)
        self.pushButtonSEQ.setFont(font)
        self.pushButtonSEQ.setAutoFillBackground(True)
        self.pushButtonSEQ.setObjectName("pushButtonSEQ")
        self.pushButtonVID = QtWidgets.QPushButton(self.groupBoxSET)
        #self.pushButtonVID.setGeometry(QtCore.QRect(10, 140, 121, 31))
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(10)
        self.pushButtonVID.setFont(font)
        self.pushButtonVID.setAutoFillBackground(True)
        self.pushButtonVID.setObjectName("pushButtonVID")
        self.pushButtonVGA = QtWidgets.QPushButton(self.groupBoxSET)
       # self.pushButtonVGA.setGeometry(QtCore.QRect(10, 190, 121, 31))
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(10)
        self.pushButtonVGA.setFont(font)
        self.pushButtonVGA.setAutoFillBackground(True)
        self.pushButtonVGA.setObjectName("pushButtonVGA")

        self.pushButtonMANUAL = QtWidgets.QPushButton(self.groupBoxSET)
        # self.pushButtonIMG.setGeometry(QtCore.QRect(10, 40, 121, 31))
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(10)
        self.pushButtonMANUAL.setFont(font)
        self.pushButtonMANUAL.setAutoFillBackground(True)
        self.pushButtonMANUAL.setObjectName("pushButtonMANUAL")




        L2_L1.addWidget(self.pushButtonIMG)
        L2_L1.addWidget(self.pushButtonSEQ)
        L2_L1.addWidget(self.pushButtonVGA)
        L2_L1.addWidget(self.pushButtonVID)
        L2_L1.addWidget(self.pushButtonMANUAL)

        self.radioButtonSVM = QtWidgets.QRadioButton(self.groupBoxSET)
       # self.radioButtonSVM.setGeometry(QtCore.QRect(200, 30, 91, 31))
        font = QtGui.QFont()
        font.setFamily("Arial Black")
        font.setPointSize(8)
        font.setBold(False)
        font.setWeight(75)
        self.radioButtonSVM.setFont(font)
        self.radioButtonSVM.setObjectName("radioButtonSVM")
        #self.radioButtonSVM.setChecked(True)
        self.radioButtonCNN = QtWidgets.QRadioButton(self.groupBoxSET)
       # self.radioButtonCNN.setGeometry(QtCore.QRect(200, 70, 91, 21))
        font = QtGui.QFont()
        font.setFamily("Arial Black")
        font.setPointSize(8)
        font.setBold(False)
        font.setWeight(75)
        self.radioButtonCNN.setFont(font)
        self.radioButtonCNN.setObjectName("radioButtonCNN")
       # self.radioButtonFUSION = QtWidgets.QRadioButton(self.groupBoxSET)
        #self.radioButtonFUSION.setGeometry(QtCore.QRect(200, 110, 111, 31))
        #font = QtGui.QFont()
        #font.setFamily("黑体")
        #font.setPointSize(10)
        #font.setBold(False)
        #font.setWeight(50)
        #self.radioButtonFUSION.setFont(font)
       # self.radioButtonFUSION.setObjectName("radioButtonFUSION")

        self.toolButton = QtWidgets.QToolButton(self.groupBoxSET)
        #self.toolButton.setGeometry(QtCore.QRect(380, 20, 111, 31))
        self.toolButton.setAutoFillBackground(False)
        self.toolButton.setStyleSheet("color: rgb(255, 170, 0);\n"
"font: 10pt \"黑体\";\n"
"")
        self.toolButton.setObjectName("toolButton")

        self.video_stop_Button = QtWidgets.QToolButton(self.groupBoxSET)
        # self.video_stop_Button.setGeometry(QtCore.QRect(380, 20, 111, 31))
        self.video_stop_Button.setAutoFillBackground(False)
        self.video_stop_Button.setStyleSheet("color: rgb(255, 170, 0);\n"
                                      "font: 10pt \"黑体\";\n"
                                      "")
        self.video_stop_Button.setObjectName("video_stop_Button")

        self.textBrowserIMGProperty = QtWidgets.QTextBrowser(self.groupBoxSET)
       # self.textBrowserIMGProperty.setGeometry(QtCore.QRect(350, 80, 161, 151))
        self.textBrowserIMGProperty.setStyleSheet("background-color: rgb(255, 255, 255);")
        self.textBrowserIMGProperty.setObjectName("textBrowserIMGProperty")


        L2_L3.addWidget(self.radioButtonSVM)
        L2_L3.addWidget(self.radioButtonCNN)
        #L2_L2.addWidget(self.radioButtonFUSION)
        L2_L3.addWidget(self.toolButton)
        L2_L3.addWidget(self.video_stop_Button)

        L2_L2.addLayout(L2_L3)
        L2_L2.addWidget(self.textBrowserIMGProperty)

        L2.addLayout(L2_L1)
        L2.addLayout(L2_L2)
        L2.addLayout(L2_L3)

        self.groupBoxRESULT = QtWidgets.QGroupBox(self.centralwidget)

       # grid.addWidget(self.groupBoxRESULT, 2, 1)
       # L3=QHBoxLayout(self.groupBoxRESULT)
       # L3_L1=QVBoxLayout()
       # L3_L2=QVBoxLayout()

        self.groupBoxRESULT.setGeometry(QtCore.QRect(20, 360, 431, 271))
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(9)
        self.groupBoxRESULT.setFont(font)
        self.groupBoxRESULT.setObjectName("groupBoxRESULT")

        self.labelFalseTarget = QtWidgets.QLabel(self.groupBoxRESULT)
        self.labelFalseTarget.setGeometry(QtCore.QRect(10, 30, 151, 21))
        palette = QtGui.QPalette()
        brush = QtGui.QBrush(QtGui.QColor(85, 255, 0))
        brush.setStyle(QtCore.Qt.SolidPattern)
        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Button, brush)
        brush = QtGui.QBrush(QtGui.QColor(85, 255, 0))
        brush.setStyle(QtCore.Qt.SolidPattern)
        palette.setBrush(QtGui.QPalette.Inactive, QtGui.QPalette.Button, brush)
        brush = QtGui.QBrush(QtGui.QColor(85, 255, 0))
        brush.setStyle(QtCore.Qt.SolidPattern)
        palette.setBrush(QtGui.QPalette.Disabled, QtGui.QPalette.Button, brush)
        self.labelFalseTarget.setPalette(palette)
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(9)
        self.labelFalseTarget.setFont(font)
        self.labelFalseTarget.setFrameShape(QtWidgets.QFrame.NoFrame)
        self.labelFalseTarget.setFrameShadow(QtWidgets.QFrame.Sunken)
        self.labelFalseTarget.setScaledContents(False)
        self.labelFalseTarget.setObjectName("labelFalseTarget")
        self.textBrowserFalseTarget = QtWidgets.QTextBrowser(self.groupBoxRESULT)
        self.textBrowserFalseTarget.setGeometry(QtCore.QRect(160, 25, 91, 31))
        self.textBrowserFalseTarget.setObjectName("textBrowserFalseTarget")

        self.labelNarrowNoise = QtWidgets.QLabel(self.groupBoxRESULT)
        self.labelNarrowNoise.setGeometry(QtCore.QRect(10, 70, 148, 21))
        self.labelNarrowNoise.setFrameShape(QtWidgets.QFrame.NoFrame)
        self.labelNarrowNoise.setFrameShadow(QtWidgets.QFrame.Sunken)
        self.labelNarrowNoise.setObjectName("labelNarrowNoise")
        self.textBrowserNarrowNoise = QtWidgets.QTextBrowser(self.groupBoxRESULT)
        self.textBrowserNarrowNoise.setGeometry(QtCore.QRect(160, 67, 91, 31))
        self.textBrowserNarrowNoise.setObjectName("textBrowserNarrowNoise")
        self.labelWideNoise = QtWidgets.QLabel(self.groupBoxRESULT)
        self.labelWideNoise.setGeometry(QtCore.QRect(18, 110, 141, 21))
        self.labelWideNoise.setFrameShape(QtWidgets.QFrame.NoFrame)
        self.labelWideNoise.setFrameShadow(QtWidgets.QFrame.Sunken)
        self.labelWideNoise.setObjectName("labelWideNoise")
        self.textBrowserWideNoise = QtWidgets.QTextBrowser(self.groupBoxRESULT)
        self.textBrowserWideNoise.setGeometry(QtCore.QRect(160, 107, 91, 31))
        self.textBrowserWideNoise.setObjectName("textBrowserWideNoise")
        self.labelScan = QtWidgets.QLabel(self.groupBoxRESULT)
        self.labelScan.setGeometry(QtCore.QRect(20, 150, 131, 21))
        self.labelScan.setFrameShape(QtWidgets.QFrame.NoFrame)
        self.labelScan.setFrameShadow(QtWidgets.QFrame.Sunken)
        self.labelScan.setObjectName("labelScan")
        self.labelComb = QtWidgets.QLabel(self.groupBoxRESULT)
        self.labelComb.setGeometry(QtCore.QRect(20, 192, 131, 21))
        self.labelComb.setFrameShape(QtWidgets.QFrame.NoFrame)
        self.labelComb.setFrameShadow(QtWidgets.QFrame.Sunken)
        self.labelComb.setObjectName("labelComb")
        self.textBrowserScanNoise = QtWidgets.QTextBrowser(self.groupBoxRESULT)
        self.textBrowserScanNoise.setGeometry(QtCore.QRect(160, 147, 91, 31))
        self.textBrowserScanNoise.setObjectName("textBrowserScanNoise")
        self.textBrowserCombNoise = QtWidgets.QTextBrowser(self.groupBoxRESULT)
        self.textBrowserCombNoise.setGeometry(QtCore.QRect(160, 188, 91, 31))
        self.textBrowserCombNoise.setObjectName("textBrowserCombNoise")
        self.label_image_show_2 = QtWidgets.QLabel(self.groupBoxRESULT)
        self.label_image_show_2.setGeometry(QtCore.QRect(260, 29, 161, 225))
        self.label_image_show_2.setStyleSheet("background-color: rgb(255, 255, 255);")
        self.label_image_show_2.setText("")
        self.label_image_show_2.setObjectName("label_image_show_2")
        # self.textEditNoiseLocation = QtWidgets.QTextEdit(self.groupBoxRESULT)
        # self.textEditNoiseLocation.setGeometry(QtCore.QRect(270, 50, 151, 61))
        # self.textEditNoiseLocation.setStyleSheet("background-color: rgb(255, 255, 127);")
        # self.textEditNoiseLocation.setObjectName("textEditNoiseLocation")
        # self.textEditNoiseParameter = QtWidgets.QTextEdit(self.groupBoxRESULT)
        # self.textEditNoiseParameter.setGeometry(QtCore.QRect(270, 140, 151, 121))
        # font = QtGui.QFont()
        # font.setFamily("宋体")
        # font.setPointSize(8)
        # self.textEditNoiseParameter.setFont(font)
        # self.textEditNoiseParameter.setStyleSheet("background-color: rgb(170, 255, 127);")
        # self.textEditNoiseParameter.setObjectName("textEditNoiseParameter")
        # self.labelNoiseLocation = QtWidgets.QLabel(self.groupBoxRESULT)
        # self.labelNoiseLocation.setGeometry(QtCore.QRect(270, 20, 91, 21))
        palette = QtGui.QPalette()
        brush = QtGui.QBrush(QtGui.QColor(85, 255, 0))
        brush.setStyle(QtCore.Qt.SolidPattern)
        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Button, brush)
        brush = QtGui.QBrush(QtGui.QColor(85, 255, 0))
        brush.setStyle(QtCore.Qt.SolidPattern)
        palette.setBrush(QtGui.QPalette.Inactive, QtGui.QPalette.Button, brush)
        brush = QtGui.QBrush(QtGui.QColor(85, 255, 0))
        brush.setStyle(QtCore.Qt.SolidPattern)
        palette.setBrush(QtGui.QPalette.Disabled, QtGui.QPalette.Button, brush)
        # self.labelNoiseLocation.setPalette(palette)
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(9)
        # self.labelNoiseLocation.setFont(font)
        # self.labelNoiseLocation.setFrameShape(QtWidgets.QFrame.NoFrame)
        # self.labelNoiseLocation.setFrameShadow(QtWidgets.QFrame.Sunken)
        # self.labelNoiseLocation.setScaledContents(False)
        # self.labelNoiseLocation.setObjectName("labelNoiseLocation")
        # self.labelNoiseParameter = QtWidgets.QLabel(self.groupBoxRESULT)
        # self.labelNoiseParameter.setGeometry(QtCore.QRect(270, 120, 91, 21))
        palette = QtGui.QPalette()
        brush = QtGui.QBrush(QtGui.QColor(85, 255, 0))
        brush.setStyle(QtCore.Qt.SolidPattern)
        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Button, brush)
        brush = QtGui.QBrush(QtGui.QColor(85, 255, 0))
        brush.setStyle(QtCore.Qt.SolidPattern)
        palette.setBrush(QtGui.QPalette.Inactive, QtGui.QPalette.Button, brush)
        brush = QtGui.QBrush(QtGui.QColor(85, 255, 0))
        brush.setStyle(QtCore.Qt.SolidPattern)
        palette.setBrush(QtGui.QPalette.Disabled, QtGui.QPalette.Button, brush)
        # self.labelNoiseParameter.setPalette(palette)
        # font = QtGui.QFont()
        # font.setFamily("华文隶书")
        # font.setPointSize(12)
        # self.labelNoiseParameter.setFont(font)
        # self.labelNoiseParameter.setFrameShape(QtWidgets.QFrame.NoFrame)
        # self.labelNoiseParameter.setFrameShadow(QtWidgets.QFrame.Sunken)
        # self.labelNoiseParameter.setScaledContents(False)
        # self.labelNoiseParameter.setObjectName("labelNoiseParameter")
        self.labelOtherNoise = QtWidgets.QLabel(self.groupBoxRESULT)
        self.labelOtherNoise.setGeometry(QtCore.QRect(20, 234, 131, 21))
        self.labelOtherNoise.setFrameShape(QtWidgets.QFrame.NoFrame)
        self.labelOtherNoise.setFrameShadow(QtWidgets.QFrame.Sunken)
        self.labelOtherNoise.setObjectName("labelOtherNoise")
        self.textBrowserOtherNoise = QtWidgets.QTextBrowser(self.groupBoxRESULT)
        self.textBrowserOtherNoise.setGeometry(QtCore.QRect(160, 228, 91, 31))
        self.textBrowserOtherNoise.setObjectName("textBrowserOtherNoise")



        self.groupBoxADV = QtWidgets.QGroupBox(self.centralwidget)
       # grid.addWidget(self.groupBoxADV, 2, 2)
        L4=QVBoxLayout(self.groupBoxADV)
        L4_L1=QHBoxLayout()
        self.groupBoxADV.setGeometry(QtCore.QRect(480, 260, 511, 370))
        font = QtGui.QFont()
        font.setFamily("黑体")
        font.setPointSize(11)
        self.groupBoxADV.setFont(font)
        self.groupBoxADV.setObjectName("groupBoxADV")

        self.stackedWidget = SlidingStackedWidget(self.groupBoxADV)
       # self.stackedWidget.setGeometry(QtCore.QRect(20, 20, 480, 280))
        self.stackedWidget.setObjectName("stackedWidget")


       #  self.startButton = QtWidgets.QPushButton(self.groupBoxADV)
       #  #self.startButton.setGeometry(QtCore.QRect(10, 310, 93, 28))
       #  self.startButton.setObjectName("startButton")
       #  self.previousButton = QtWidgets.QPushButton(self.groupBoxADV)
       # # self.previousButton.setGeometry(QtCore.QRect(130, 310, 93, 28))
       #  self.previousButton.setObjectName("previousButton")
       #  self.nextButton = QtWidgets.QPushButton(self.groupBoxADV)
       # # self.nextButton.setGeometry(QtCore.QRect(250, 310, 93, 28))
       #  self.nextButton.setObjectName("nextButton")
       #  self.stopButton = QtWidgets.QPushButton(self.groupBoxADV)
       # # self.stopButton.setGeometry(QtCore.QRect(370, 310, 93, 28))
       #  self.stopButton.setObjectName("stopButton")

        # L4_L1.addWidget(self.startButton)
        # L4_L1.addWidget(self.previousButton)
        # L4_L1.addWidget(self.nextButton)
        # L4_L1.addWidget(self.stopButton)

        L4.addWidget(self.stackedWidget)
        L4.addLayout(L4_L1)

        # 添加图片页面
        for name in os.listdir('data2'):
            label = QLabel(self.stackedWidget)
            label.setScaledContents(True)
            label.setPixmap(QtGui.QPixmap('data2/' + name))
            self.stackedWidget.addWidget(label)


        self.stackedWidget.autoStart()

        HQ16pATR.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(HQ16pATR)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 1030, 26))
        self.menubar.setObjectName("menubar")
        # self.menuFile = QtWidgets.QMenu(self.menubar)
        # self.menuFile.setObjectName("menuFile")
        self.menuTrain = QtWidgets.QMenu(self.menubar)
        self.menuTrain.setObjectName("menuTrain")
        self.menuTest = QtWidgets.QMenu(self.menubar)
        self.menuTest.setObjectName("menuTest")
        self.menuEvaluation = QtWidgets.QMenu(self.menubar)
        self.menuEvaluation.setObjectName("menuEvaluation")
        self.menuManual = QtWidgets.QMenu(self.menubar)
        self.menuManual.setObjectName("menuManual")
        self.menuSettings = QtWidgets.QMenu(self.menubar)
        self.menuSettings.setObjectName("menuSettings")
        HQ16pATR.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(HQ16pATR)
        self.statusbar.setObjectName("statusbar")
        HQ16pATR.setStatusBar(self.statusbar)
        self.actionshape = QtWidgets.QAction(HQ16pATR)
        self.actionshape.setObjectName("actionshape")
        self.actionIMGFile = QtWidgets.QAction(HQ16pATR)
        self.actionIMGFile.setObjectName("actionIMGFile")
        self.actionIMGFolder = QtWidgets.QAction(HQ16pATR)
        self.actionIMGFolder.setObjectName("actionIMGFolder")
        self.actiondefense_multisource = QtWidgets.QAction(HQ16pATR)
        self.actiondefense_multisource.setObjectName("actiondefense_multisource")
        self.actionCNNtrain = QtWidgets.QAction(HQ16pATR)
        self.actionCNNtrain_fugaishi = QtWidgets.QAction(HQ16pATR)
        self.actionCNNtrain.setObjectName("actionCNNtrain")
        self.actionCNNtrain_fugaishi.setObjectName("actionCNNtrain_fugaishi")
        self.actionFUSIONtrain = QtWidgets.QAction(HQ16pATR)
        self.actionFUSIONtrain.setObjectName("actionFUSIONtrain")
        self.action_2 = QtWidgets.QAction(HQ16pATR)
        self.action_2.setObjectName("action_2")
        self.action_6 = QtWidgets.QAction(HQ16pATR)
        self.action_6.setObjectName("action_6")
        self.action_7 = QtWidgets.QAction(HQ16pATR)
        self.action_7.setObjectName("action_7")
        self.actionstandardtest = QtWidgets.QAction(HQ16pATR)
        self.actionstandardtest.setObjectName("actionstandardtest")
        self.actionadvtest = QtWidgets.QAction(HQ16pATR)
        self.actionadvtest.setObjectName("actionadvtest")
        self.actionMultisource_adv_classification = QtWidgets.QAction(HQ16pATR)
        self.actionMultisource_adv_classification.setObjectName("actionadvtest")

        self.actionFusiontest = QtWidgets.QAction(HQ16pATR)
        self.actionFusiontest.setObjectName("actionFusiontest")
        self.actionDNN = QtWidgets.QAction(HQ16pATR)
        self.actionDNN.setObjectName("actionDNN")
        self.actionadversarialattack = QtWidgets.QAction(HQ16pATR)
        self.actionadversarialattack.setObjectName("actionadversarialattack")
        self.actionblackattack = QtWidgets.QAction(HQ16pATR)
        self.actionblackattack.setObjectName("actionblackattack")
        self.actionsparseattack = QtWidgets.QAction(HQ16pATR)
        self.actionsparseattack.setObjectName("actionsparseattack")

        self.actionCAM = QtWidgets.QAction(HQ16pATR)
        self.actionCAM.setObjectName("actionCAM")

        # self.actionCAMvideo_recording = QtWidgets.QAction(HQ16pATR)
        # self.actionCAMvideo_recording.setObjectName("CAMactionvideo_recording")
        # self.actionCAMimageset_recording = QtWidgets.QAction(HQ16pATR)
        # self.actionCAMimageset_recording.setObjectName("CAMactionimageset_recording")

        self.actionVGA = QtWidgets.QAction(HQ16pATR)
        self.actionVGA.setObjectName("actionVGA")

        # self.actionVGAvideo_recording = QtWidgets.QAction(HQ16pATR)
        # self.actionVGAvideo_recording.setObjectName("VGAactionvideo_recording")
        # self.actionVGAimageset_recording = QtWidgets.QAction(HQ16pATR)
        # self.actionVGAimageset_recording.setObjectName("VGAactionimageset_recording")

        self.actionVIDEO = QtWidgets.QAction(HQ16pATR)
        self.actionVIDEO.setObjectName("actionVIDEO")
        self.actiondefense_singlesource = QtWidgets.QAction(HQ16pATR)
        self.actiondefense_singlesource.setObjectName("actiondefense_singlesource")
        self.actiondefense_valsource = QtWidgets.QAction(HQ16pATR)
        self.actiondefense_valsource.setObjectName("actiondefense_valsource")
        self.actionmodel = QtWidgets.QAction(HQ16pATR)
        self.actionmodel.setObjectName("actionmodel")
        self.actiondetector = QtWidgets.QAction(HQ16pATR)
        self.actiondetector.setObjectName("actiondetector")
        # self.menuFile.addAction(self.actionCAM)
        # # self.menuFile.addAction(self.actionCAMvideo_recording)
        # # self.menuFile.addAction(self.actionCAMimageset_recording)
        # self.menuFile.addAction(self.actionVGA)
        # # self.menuFile.addAction(self.actionVGAvideo_recording)
        # # self.menuFile.addAction(self.actionVGAimageset_recording)
        # self.menuFile.addSeparator()
        # self.menuFile.addAction(self.actionVIDEO)
        # self.menuFile.addSeparator()
        # self.menuFile.addAction(self.actionIMGFile)
        # self.menuFile.addAction(self.actionIMGFolder)
        self.menuTrain.addAction(self.actionCNNtrain)
        self.menuTrain.addAction(self.actionCNNtrain_fugaishi)
        #self.menuTrain.addAction(self.actionFUSIONtrain)
        self.menuTest.addAction(self.actionstandardtest)
        self.menuTest.addAction(self.actionadvtest)
        self.menuTest.addAction(self.actionMultisource_adv_classification)
        #self.menuTest.addAction(self.actionFusiontest)
        self.menuEvaluation.addAction(self.actionadversarialattack)
        self.menuEvaluation.addAction(self.actionblackattack)
        self.menuEvaluation.addAction(self.actionsparseattack)
        self.menuManual.addAction(self.actiondefense_singlesource)
        self.menuManual.addAction(self.actiondefense_valsource)
        self.menuManual.addAction(self.actiondefense_multisource)

        #self.menuSettings.addAction(self.actiondefense_singlesource)
        self.menuSettings.addAction(self.actionmodel)
        self.menuSettings.addAction(self.actiondetector)
        # self.menubar.addAction(self.menuFile.menuAction())
        self.menubar.addAction(self.menuTrain.menuAction())
        self.menubar.addAction(self.menuTest.menuAction())
        self.menubar.addAction(self.menuEvaluation.menuAction())
        self.menubar.addAction(self.menuManual.menuAction())
        self.menubar.addAction(self.menuSettings.menuAction())
        self.openimage_exp(self.label_image_show_2,'result_image/2.png')
        #video
        self.videoPath="D:/"
        self.videoCapture=cv2.VideoCapture()
        self.timer = QTimer()


        self.retranslateUi(HQ16pATR)
        self.stackedWidget.setCurrentIndex(1)
        QtCore.QMetaObject.connectSlotsByName(HQ16pATR)

        self.if_OPenCAM = 0
        self.if_OPenCAM_video_recording  = 0
        self.if_OPenCAM_imageset_recording = 0

        self.if_OPenVGA = 0
        self.if_OPenVGA_video_recording  = 0
        self.if_OPenVGA_imageset_recording = 0
     ### Support functions

    def setupVariables(self, videoPath):

        # True at the end if all went well
        success = False

        try:
            ### Video path
            self.videoPath = videoPath

            ### OpenCV video capture
            # Select file to capture
            self.videoCapture = cv2.VideoCapture(self.videoPath)
            # Get video FPS
            self.videoCapture_fps = self.videoCapture.get(cv2.CAP_PROP_FPS)
            # Get video number of frames
            self.videoCapture_nFrame = self.videoCapture.get(cv2.CAP_PROP_FRAME_COUNT)
            # Get video frame width
            self.videoCapture_frameWidth = self.videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)
            # Get video frame height
            self.videoCapture_frameHeight = self.videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)
            # Video duration (in seconds)
            self.duration = int(self.videoCapture_nFrame / self.videoCapture_fps)

            # print(self.videoCapture_fps,self.videoCapture_nFrame,'yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy')1.0,50

            ### VideoPlayer speed
            self.speed = 1.0
            self.timer = QTimer()
            self.i =0
            self.timer.timeout.connect(self.nextFrameSlot)
            self.timer.timeout.connect(self.i_bian)
            self.timer.setInterval((1000. / self.videoCapture_fps) * self.speed)
            self.timer.start()
            success = True
        except Exception as e:
            success = False
        # msg = QMessageBox()
        # msg.setIcon(QMessageBox.Critical)
        # msg.setText(str(e.__class__))
        # msg.setDetailedText(str(e))
        # msg.setWindowTitle("VideoPlayerOpenCV ERROR")
        # msg.setStandardButtons(QMessageBox.Ok)
        # msg.exec_()


        return success

    def i_bian(self):
        self.i = 5
    def OPenVideo(self):


        self.video_filenames, imgType = QtWidgets.QFileDialog.getOpenFileName(self.centralwidget, "openVideo", "",
                                                                   "All Files(*)")

        self.setupVariables(self.video_filenames)

        #self.videoPlayerControlBar.enablePlayButton(True)
        self.videoPlayerControlBar.enableButtons(True)
        """
        cap=cv2.VideoCapture(filenames)
        #cap = cv2.VideoCapture('test.mp4')
        # cap = cv2.VideoCapture(0)
        fps = 24
        while cap.isOpened():

            ret, frame = cap.read()
            # opencv 默认图像格式是rgb qimage要使用BRG,这里进行格式转换
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

            # 测试简单的图像处理
            # img = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)  # 转化为灰度图
            # blur = cv2.GaussianBlur(img, (3, 3), 0)  # 用高斯滤波处理原图像降噪
            # canny = cv2.Canny(blur, 50, 150)  # 50是最小阈值,150是最大阈值

            # mat-->qimage
            a = QImage(frame.data, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
            self.IMGDisplay.setPixmap(QPixmap.fromImage(a))
            self.IMGDisplay.show()

            # cv2.waitKey(int(1000 / fps))
            if cv2.waitKey(1000) & 0xFF == ord('q'):
                break

        cap.release()
        """

    def OPenVGA(self):

        self.if_OPenVGA = 1 - self.if_OPenVGA
        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)

        fps = 12
        size = (640, 480)
        fourcc = cv2.VideoWriter_fourcc(*"XVID")

        i = 0
        tricks = time.time()
        if self.if_OPenVGA == 1:
            cap.set(cv2.CAP_PROP_FPS, 24)
            videoWrite = cv2.VideoWriter('videos/VGA_{}.avi'.format(tricks), fourcc, fps,
                                         size)  # 根据图片的大小，创建写入对象 （文件名，支持的编码器，5帧，视频大小（图片大小））
        while cap.isOpened() and self.if_OPenVGA == 1:
            ret, frame = cap.read()
            # opencv 默认图像格式是rgb qimage要使用BRG,这里进行格式转换

          #  frame = cv2.flip(frame, 1)

            ############################此顺序保证直接显示和储存都正常
            videoWrite.write(frame)
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            if self.radioButtonCNN.isChecked():
                self.CNN_cam_image_test(frame, self.CNN_image_test_model_dir)
            elif self.radioButtonSVM.isChecked():
                self.conventional_cam_image_test(
                    self.conventional_setting.conventional_classifier if self.if_conventional_setted else "dt",
                    frame)

            # 测试简单的图像处理
            # img = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)  # 转化为灰度图
            # blur = cv2.GaussianBlur(img, (3, 3), 0)  # 用高斯滤波处理原图像降噪
            # canny = cv2.Canny(blur, 50, 150)  # 50是最小阈值,150是最大阈值

            # mat-->qimage
            a = QImage(frame.data, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
            self.IMGDisplay.setPixmap(QPixmap.fromImage(a))
            self.IMGDisplay.show()

            cv2.waitKey(int(1000 / fps))
            # if cv2.waitKey(1000) & 0xFF == ord('q'):
            #    break
            # print(frame.shape)

            i += 1
            print(i)
        if self.if_OPenVGA == 0:
            cap.release()
            # videoWrite.release()

    def OPenVGA_video_recording(self):
        self.if_OPenVGA_video_recording = 1 - self.if_OPenVGA_video_recording
        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)

        fps = 12
        size = (640, 480)

        fourcc = cv2.VideoWriter_fourcc(*"XVID")

        i = 0
        tricks = time.time()
        if self.if_OPenVGA_video_recording == 1:
            cap.set(cv2.CAP_PROP_FPS, 24)
            videoWrite = cv2.VideoWriter('videos/VGA_{}.avi'.format(tricks), fourcc, fps,
                                         size)  # 根据图片的大小，创建写入对象 （文件名，支持的编码器，5帧，视频大小（图片大小））
        while cap.isOpened() and self.if_OPenVGA_video_recording == 1:
            ret, frame = cap.read()
            # opencv 默认图像格式是rgb qimage要使用BRG,这里进行格式转换

           # frame = cv2.flip(frame, 1)

            ############################此顺序保证直接显示和储存都正常
            videoWrite.write(frame)
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

            # mat-->qimage
            a = QImage(frame.data, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
            self.IMGDisplay.setPixmap(QPixmap.fromImage(a))
            self.IMGDisplay.show()

            cv2.waitKey(int(1000 / fps))
            # if cv2.waitKey(1000) & 0xFF == ord('q'):
            #    break
            # print(frame.shape)

            i += 1
            print(i)
        if self.if_OPenVGA_video_recording == 0:
            cap.release()
            # videoWrite.release()

    def OPenVGA_imageset_recording(self):
        self.if_OPenVGA_imageset_recording = 1 - self.if_OPenVGA_imageset_recording
        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
        fps = 24
        tricks = time.time()
        if self.if_OPenVGA_imageset_recording == 1:
            os.mkdir('imageset/VGA_'+str(tricks))
        i = 0
        frame_skip = 1
        while cap.isOpened() and self.if_OPenVGA_imageset_recording == 1:
            ret, frame = cap.read()
            # opencv 默认图像格式是rgb qimage要使用BRG,这里进行格式转换

           # frame = cv2.flip(frame, 1)

            if i%frame_skip == 0:
                cv2.imwrite('imageset/VGA_'+str(tricks)+'/{}.png'.format(int(i/frame_skip)),frame)
            ############################此顺序保证直接显示和储存都正常
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

            # mat-->qimage
            a = QImage(frame.data, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
            self.IMGDisplay.setPixmap(QPixmap.fromImage(a))
            self.IMGDisplay.show()

            cv2.waitKey(int(1000 / fps))

            # if cv2.waitKey(1000) & 0xFF == ord('q'):
            #    break
            # print(frame.shape)
            i+=1


        if self.if_OPenVGA_imageset_recording == 0:
            cap.release()
            # videoWrite.release()

    def OPenCAM(self):
        self.if_OPenCAM = 1 - self.if_OPenCAM
        print(self.if_OPenCAM,'ddddddddddddddddddd')

        #只有摄像头时，ID 为0，
        #有VGA卡时，ID为1
        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)




        fps = 12
        size = (640, 480)

        fourcc = cv2.VideoWriter_fourcc(*"XVID")

        i=0
        tricks = time.time()
        if self.if_OPenCAM==1:
            print('aaaaaaaaaaaaaaa')
            cap.set(cv2.CAP_PROP_FPS, 24)
            videoWrite = cv2.VideoWriter('videos/CAM_{}.avi'.format(tricks), fourcc, fps, size)  # 根据图片的大小，创建写入对象 （文件名，支持的编码器，5帧，视频大小（图片大小））
        print('sssssssssssss',cap.isOpened())
        while cap.isOpened() and self.if_OPenCAM==1:
            # print('dddddddddddddddddddddd')
            ret, frame = cap.read()
            # opencv 默认图像格式是rgb qimage要使用BRG,这里进行格式转换

            # frame = cv2.flip(frame,1)


############################此顺序保证直接显示和储存都正常
            videoWrite.write(frame)
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            if self.radioButtonCNN.isChecked():
                self.CNN_cam_image_test(frame,self.CNN_image_test_model_dir)
            elif self.radioButtonSVM.isChecked():
                self.conventional_cam_image_test(self.conventional_setting.conventional_classifier if self.if_conventional_setted else "dt",frame)

            # 测试简单的图像处理
            # img = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)  # 转化为灰度图
            # blur = cv2.GaussianBlur(img, (3, 3), 0)  # 用高斯滤波处理原图像降噪
            # canny = cv2.Canny(blur, 50, 150)  # 50是最小阈值,150是最大阈值

            # mat-->qimage
            a = QImage(frame.data, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
            self.IMGDisplay.setPixmap(QPixmap.fromImage(a))
            self.IMGDisplay.show()



            cv2.waitKey(int(1000 / fps))
            # if cv2.waitKey(1000) & 0xFF == ord('q'):
            #    break
            # print(frame.shape)

            i+=1
            print(i)
        if self.if_OPenCAM==0:
            cap.release()
            # videoWrite.release()

    def OPenCAM_video_recording(self):
            self.if_OPenCAM_video_recording = 1 - self.if_OPenCAM_video_recording
            cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)

            fps = 12
            size = (640, 480)

            fourcc = cv2.VideoWriter_fourcc(*"XVID")

            i = 0
            tricks = time.time()
            if self.if_OPenCAM_video_recording == 1:
                cap.set(cv2.CAP_PROP_FPS, 24)
                videoWrite = cv2.VideoWriter('videos/CAM_{}.avi'.format(tricks), fourcc, fps,
                                             size)  # 根据图片的大小，创建写入对象 （文件名，支持的编码器，5帧，视频大小（图片大小））
            while cap.isOpened() and self.if_OPenCAM_video_recording == 1:
                ret, frame = cap.read()
                # opencv 默认图像格式是rgb qimage要使用BRG,这里进行格式转换

                frame = cv2.flip(frame, 1)

                ############################此顺序保证直接显示和储存都正常
                videoWrite.write(frame)
                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

                # mat-->qimage
                a = QImage(frame.data, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
                self.IMGDisplay.setPixmap(QPixmap.fromImage(a))
                self.IMGDisplay.show()

                cv2.waitKey(int(1000 / fps))
                # if cv2.waitKey(1000) & 0xFF == ord('q'):
                #    break
                # print(frame.shape)

                i += 1
                print(i)
            if self.if_OPenCAM_video_recording == 0:
                cap.release()
                # videoWrite.release()


    def OPenCAM_imageset_recording(self):
            self.if_OPenCAM_imageset_recording = 1 - self.if_OPenCAM_imageset_recording
            cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
            fps = 24
            tricks = time.time()
            if self.if_OPenCAM_imageset_recording == 1:
                os.mkdir('imageset/CAM_'+str(tricks))
            i = 0
            frame_skip = 1
            while cap.isOpened() and self.if_OPenCAM_imageset_recording == 1:
                ret, frame = cap.read()
                # opencv 默认图像格式是rgb qimage要使用BRG,这里进行格式转换

                frame = cv2.flip(frame, 1)

                if i%frame_skip == 0:
                    cv2.imwrite('imageset/CAM_'+str(tricks)+'/{}.png'.format(int(i/frame_skip)),frame)
                ############################此顺序保证直接显示和储存都正常
                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

                # mat-->qimage
                a = QImage(frame.data, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
                self.IMGDisplay.setPixmap(QPixmap.fromImage(a))
                self.IMGDisplay.show()

                cv2.waitKey(int(1000 / fps))

                # if cv2.waitKey(1000) & 0xFF == ord('q'):
                #    break
                # print(frame.shape)
                i+=1


            if self.if_OPenCAM_imageset_recording == 0:
                cap.release()
                # videoWrite.release()

    # # 可以读取带中文路径的图
    # def cv_imread(file_path, type=0):
    #     import numpy as np
    #     cv_img = cv2.imdecode(np.fromfile(file_path, dtype=np.uint8), -1)
    #     if (type == 0):
    #         cv_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2GRAY)
    #     return cv_img

    def OPenIMG(self):
        if self.radioButtonCNN.isChecked() == True:
            filenames, imgType = QtWidgets.QFileDialog.getOpenFileName(self.centralwidget, "openImage", "Data/场景/UCMerced_LandUse/val",
                                                                   "*.jpg;;*.png;;*.tif;;*.jpeg;;All Files(*)")
        elif self.radioButtonSVM.isChecked() == True:
            filenames, imgType = QtWidgets.QFileDialog.getOpenFileName(self.centralwidget, "openImage",
                                                                       r"D:\Data\adv_fool_pic\UC_adv_fool",
                                                                       "*.jpg;;*.png;;*.tif;;*.jpeg;;All Files(*)")
        filenames.replace('\\','/')
        # print(filenames)

        # img = cv2.imread(filenames, 1)
        # # opencv 默认图像格式是rgb qimage要使用BRG,这里进行格式转换
        # img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        #
        # # mat-->qimage
        # a = QImage(img.data, img.shape[1], img.shape[0], QImage.Format_RGB888)
        # self.IMGDisplay.setPixmap(QPixmap.fromImage(a))

        jpg = QtGui.QPixmap(filenames).scaled(self.IMGDisplay.width(), self.IMGDisplay.height())
        self.IMGDisplay.setPixmap(jpg)
        self.IMGDisplay.repaint()
        self.IMGDisplay.show()
        self.image_filename = filenames
        self.if_image_opened = False

    def OPenIMGforManual(self):

        filenames, imgType = QtWidgets.QFileDialog.getOpenFileName(self.centralwidget, "openImage", "",
                                                                   "*.jpg;;*.png;;All Files(*)")


        w = ImageView.ImageView(image=filenames, background=Qt.black)
        #w.setupUi()
        w.show()

    def OPenIMGFolder(self):
        """  file_path = QtWidgets.QFileDialog.getExistingDirectory(self, '选择文文件夹', '/')"""
        # w = ImageView(image='background.jpg', background=Qt.black)
        # w.show()
        #self.PIC.setPixmap('background.jpg')
        #self.PIC.show()

    def autoStart(self):
        self.previousButton.setEnabled(False)
        self.nextButton.setEnabled(False)
        self.stackedWidget.autoStart()

    def autoStop(self):
        self.nextButton.setEnabled(True)
        self.previousButton.setEnabled(True)
        self.stackedWidget.autoStop()

    ### Functions of VideoPlayer

    # Load next frame
    def nextFrameSlot(self):
        ret, frame = self.videoCapture.read()

        if (ret == True):
            # OpenCV yields frames in BGR format
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # print(self.i)
            # # if self.radioButtonSVM.isChecked():
            # #     self.conventional_image_test(self.conventional_setting.conventional_classifier if self.if_conventional_setted else "dt",self.video_filenames,frame,self.i)
            # # elif self.radioButtonCNN.isChecked():
            # print(self.CNN_image_test_model_dir)
            # print(self.data_name)
            self.CNN_image_test(frame,self.CNN_image_test_model_dir,self.i, self.data_name)

            # Show frame in videoFrame
            img = QImage(frame, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
            pix = QPixmap.fromImage(img)
            self.IMGDisplay.setPixmap(pix)


        # Show annotations
        # self.mw.setupAnnotations(2, "", self.videoCapture.get(cv2.CAP_PROP_POS_FRAMES))
        else:
            self.pause()

    # def conventional_image_test(self,conventional_img_classifier,video_test_address,frame,i):
    #     ##随机打乱
    #
    #     img = transform.resize(frame, (600, 600))
    #
    #
    #     if img.shape[0] != 3:
    #         # plt.imshow(img, cmap='gray')
    #
    #         img = img[np.newaxis,]
    #     # else:
    #     #     plt.imshow(img)
    #     # plt.show()
    #     img = np.reshape(img, [img.shape[0], img.shape[1] * img.shape[2] * img.shape[3]])
    #     img = img/255
    #     img = data.mean_wise(img)
    #     pca_trans = joblib.load('pths/pca_trans.pth')
    #     X_test = pca_trans.transform(img)
    #     classifier = joblib.load('pths/{}.pth'.format(conventional_img_classifier))
    #     test_result = model.test(X_test,classifier)
    #
    #     # print(dataset)
    #
    #     sub_dir = ['J0','J1','J2','J3','J4','J5','J6','J7','J8','J9']
    #     # print(sub_dir)
    #     if i == 0 :
    #         imagetest_out_log=['传统方法视频数据识别： ','\n','识别结果： '+sub_dir[int(test_result)]]
    #         imagetest_out_log2 = ['传统方法视频识别： ',  '视频地址： ' + video_test_address,
    #                              '识别结果： ' + sub_dir[int(test_result)]]
    #
    #         write_excel_xlsx('log.xlsx', imagetest_out_log2)
    #
    #     else:
    #         imagetest_out_log = [ '识别结果： ' + sub_dir[int(test_result)]]
    #         imagetest_out_log2 = [
    #                               '识别结果： ' + sub_dir[int(test_result)]]
    #
    #         write_excel_xlsx('log.xlsx', imagetest_out_log2,tiao=-1)
    #     out1 = ''.join(imagetest_out_log)
    #     a = random.randint(0, 360)
    #     b = random.randint(0, 360) / 10
    #
    #     c = random.randint(40, 80) * 50
    #     d = random.randint(10, 30)
    #     e = random.randint(10, 20)
    #     f = random.randint(8, 20) * 10
    #
    #
    #     self.textBrowserIMGProperty.setText(out1)
    #     self.conventional_zhihxindu(sub_dir[int(test_result)])
    #     self.textEditNoiseLocation.setText('方位{}度\n距离{}km'.format(a, b))
    #     self.textEditNoiseParameter.setText('中心频率{}MHZ\n带宽{}MHZ\n干扰强度{}dB\n干扰功率{}W'.format(c, d, e, f))

    def openimage_exp(self, image_show, img_address):
        imgName = img_address
        jpg = QtGui.QPixmap(imgName).scaled(image_show.width(), image_show.height())
        image_show.setPixmap(jpg)

    ### ACTIONS: VideoPlayerControlBar -> VideoPlayerOpenCV
    def CNN_image_test(self,frame,model_address,k,dataset='uc'):
        if dataset == 'uc':
            sub_dir = os.listdir('Data/场景/UCMerced_LandUse/val')
            print(sub_dir)
            data_transforms = transforms.Compose([
                                transforms.ToPILImage(),
                                transforms.Resize((224, 224)),
                                transforms.ToTensor(),
                                transforms.Normalize((0.44979182,  0.48921227, 0.48212156), (0.19673954, 0.20322968, 0.21901236))])
            # mean0, mean1, mean2, std0, std1, std2 = float(0.485), float(0.456), float(0.406), float(0.229), float(
            #     0.224), float(0.225)
        elif dataset == 'mstar':
            # image_shape = (128,128)
            sub_dir = os.listdir('Data/目标/MSTAR-10/val')
            data_transforms = transforms.Compose([
                transforms.Grayscale(1),
                transforms.CenterCrop(128),
                transforms.ToTensor(),
                transforms.Normalize([0.184, ], [0.119, ])
            ])
            # mean0 = 0.184
            # std0 = 0.119
        # img = transform.resize(frame, image_shape)
        # img_yuan = img
        # img = img / 255
        print(model_address)
        frame = data_transforms(frame)
        # frame = np.transpose(frame, (2, 0, 1))
        frame_tensor = frame.unsqueeze(0)
        frame_tensor = frame_tensor.float()
        print(frame_tensor.shape)    # torch.Size([1, 3, 224, 224])
        dataloders = DataLoader(dataset=frame_tensor, batch_size=1, shuffle=True)
        # for data_test in dataloders:
        #     test_images, test_labels = data_test
        model = torch.load(model_address)
        use_gpu = torch.cuda.is_available()
        model.eval()
        for data in dataloders:
            inputs = data
        # wrap them in Variable
            if use_gpu:
                inputs = Variable(inputs.cuda())
            else:
                inputs = Variable(inputs)

            result = model(inputs)[0]
            print(result)
            result = F.softmax(result)

        # print(channel)

        # if channel == 3:
        #     img = np.transpose(img, (2, 0, 1))
        #     img[0] = (img[0]-mean0 )/std0
        #     img[1] = (img[1]-mean1 )/std1
        #     img[2] = (img[2]-mean2 )/std2
        #
        # else:
        #     img = (img-mean0)/std0
        #     img = img[np.newaxis,:,:]
        # # img = img[:,(self.resize-self.centercrop)//2:(self.resize+self.centercrop)//2,(self.resize-self.centercrop)//2:(self.resize+self.centercrop)//2]
        #
        # img = img[np.newaxis,:,:,:]
        # img = np.float32(img)
        #
        # img = torch.from_numpy(img)
        # img = img.to(device)
        # model = torch.load(model_address)
        # model = model.to(device)

        # result = model(img)[0]
        # print(result)
        # result = F.softmax(result)
        # result = torch.exp(result/22.4)
        # result = result / torch.sum(result)

        out = []
        # if dataset == "uc":
        #     sub_dir = ['中等居民区', '停车场', '储水厂', '农田', '十字路口', '可移动住宅', '密集居民区', '建筑物', '树丛', '棒球场', "森林", "沙滩", "河流",
        #                "海港", "稀疏居民区", "立交桥", "网球场", "飞机", "飞机跑道", "高尔夫球场", "高速公路"]
        # elif dataset == "mstar":
        #     sub_dir = ["2S1", "BMP2", "BRDM_2", "BTR60", "BTR70", "D7", "T62", "T72", "ZIL131", "ZSU_23_4"]
        # out.extend(['CNN视频数据识别:','\n','模型地址为:',model_address])
        # out.extend(['\n','识别结果为：',sub_dir[int(torch.max(result,0).indices)],'\n','分类置信度：'])
        # pred = sub_dir[int(torch.max(result,0).indices)]
        # for i in range(len(sub_dir)):
        #     out.extend([sub_dir[i],':',float('%.2g' % result[i].cpu().detach().numpy()),' '])
        #     # if i == 4:
        #     #     self.out.extend('\n            ')
        # out = [str(i) for i in out]
        # out = ''.join(out)
        # CNN_image_test_out_log = out.split('\n')
        # write_excel_xlsx('log.xlsx', CNN_image_test_out_log)
        if dataset == "uc":
            sub_dir = ['农田', "飞机", '棒球场', "沙滩",'建筑物', '树丛', '密集居民区', "森林", "高速公路",
                       "高尔夫球场", "海港", '十字路口', '中等居民区', '可移动住宅', "立交桥", '停车场', "河流", "飞机跑道", "稀疏居民区", '储水厂', "网球场"]
        elif dataset == "mstar":
            sub_dir = ["2S1", "BMP2", "BRDM_2", "BTR60", "BTR70", "D7", "T62", "T72", "ZIL131", "ZSU_23_4"]
        out.extend(['\n','识别结果为：',sub_dir[int(torch.max(result,0).indices)],'\n','分类置信度：'])
        pred = sub_dir[int(torch.max(result,0).indices)]
        for i in range(len(sub_dir)):
            out.extend([sub_dir[i],':',float('%.2g' % result[i].cpu().detach().numpy()),' '])
            # if i == 4:
            #     self.out.extend('\n            ')
        out = [str(i) for i in out]
        out = ''.join(out)
        CNN_image_test_out_log = out.split('\n')
        write_excel_xlsx('log.xlsx', CNN_image_test_out_log,tiao=-1)
        result = result.cpu().detach().numpy()
        self.textBrowserIMGProperty.setText(out)
        self.textBrowserIMGProperty.repaint()
        self.CNN_zhixindu(result)
        # a = random.randint(0, 360)
        # b = random.randint(0, 360) / 10
        #
        # c = random.randint(40, 80) * 50
        # d = random.randint(10, 30)
        # e = random.randint(10, 20)
        # f = random.randint(8, 20) * 10
        # self.textEditNoiseLocation.setText('方位{}度\n距离{}km'.format(a, b))
        # self.textEditNoiseParameter.setText('中心频率{}MHZ\n带宽{}MHZ\n干扰强度{}dB\n干扰功率{}W'.format(c, d, e, f))

    def CNN_cam_image_test(self,frame,model_address,dataset='uc'):
        if dataset == 'uc':
            image_shape = (224, 224)
            channel = 3
            sub_dir = os.listdir('Data/场景/UCMerced_LandUse/val')
            mean0, mean1, mean2, std0, std1, std2 = float(0.44979182), float(0.48921227), float(0.48212156), float(0.19673954), float(
                0.20322968), float(0.21901236)
        elif dataset == 'mstar':
            image_shape = (128, 128)
            channel = 1
            sub_dir = os.listdir('Data/目标/MSTAR-10/val')
            mean0 = 0.184
            std0 = 0.119
        img = transform.resize(frame, image_shape)
        if channel == 3:
            img = np.transpose(img, (2, 0, 1))
            img[0] = (img[0]-mean0 )/std0
            img[1] = (img[1]-mean1 )/std1
            img[2] = (img[2]-mean2 )/std2

        else:
            img = (img-mean0)/std0
            img = img[np.newaxis,:,:]
        # img = img[:,(self.resize-self.centercrop)//2:(self.resize+self.centercrop)//2,(self.resize-self.centercrop)//2:(self.resize+self.centercrop)//2]

        img = img[np.newaxis,:,:,:]
        img = np.float32(img)

        img = torch.from_numpy(img)
        img = img.to(device)
        model = torch.load(model_address)
        model = model.to(device)

        result = model(img)[0]
        print(result)
        # result = F.softmax(result)
        result = torch.exp(result/2.7183)
        result = result / torch.sum(result)

        # sub_dir = ['J0','J1','J2','J3','J4','J5','J6','J7','J8','J9']
        out = []

        out.extend(['\n','识别结果为：',sub_dir[int(torch.max(result,0).indices)],'\n','分类置信度：'])
        pred = sub_dir[int(torch.max(result,0).indices)]
        for i in range(len(sub_dir)):
            out.extend([sub_dir[i],':',float('%.2g' % result[i].cpu().detach().numpy()),' '])
            # if i == 4:
            #     self.out.extend('\n            ')
        out = [str(i) for i in out]
        out = ''.join(out)
        CNN_image_test_out_log = out.split('\n')
        # write_excel_xlsx('log.xlsx', CNN_image_test_out_log,tiao=-1)
        result = result.cpu().detach().numpy()
        self.textBrowserIMGProperty.setText(out)
        self.CNN_zhixindu(result)
        # a = random.randint(0, 360)
        # b = random.randint(0, 360) / 10
        #
        # c = random.randint(40, 80) * 50
        # d = random.randint(10, 30)
        # e = random.randint(10, 20)
        # f = random.randint(8, 20) * 10
        # self.textEditNoiseLocation.setText('方位{}度\n距离{}km'.format(a, b))
        # self.textEditNoiseParameter.setText('中心频率{}MHZ\n带宽{}MHZ\n干扰强度{}dB\n干扰功率{}W'.format(c, d, e, f))

    def conventional_cam_image_test(self,conventional_img_classifier,frame,dataset='uc'):
        ##随机打乱
        if dataset == 'uc':
            image_shape = (224, 224)
            channel = 3
            sub_dir = os.listdir('Data/场景/UCMerced_LandUse/val')
            mean0, mean1, mean2, std0, std1, std2 = float(0.44979182), float(0.48921227), float(0.48212156), float(0.19673954), float(
                0.20322968), float(0.21901236)
        elif dataset == 'mstar':
            image_shape = (128, 128)
            channel = 1
            sub_dir = os.listdir('Data/目标/MSTAR-10/val')
            mean0 = 0.184
            std0 = 0.119
        img = transform.resize(frame, image_shape)



        if channel == 1:
            # plt.imshow(img, cmap='gray')

            img = img[np.newaxis,]
        # else:
        #     plt.imshow(img)
        # plt.show()
        img = np.reshape(img, [img.shape[0], img.shape[1] * img.shape[2] * img.shape[3]])
        img = img/255
        img = data.mean_wise(img)
        pca_trans = joblib.load('pths/pca_trans.pth')
        X_test = pca_trans.transform(img)
        classifier = joblib.load('pths/{}.pth'.format(conventional_img_classifier))
        test_result = model.test(X_test,classifier)

        # print(dataset)

        # sub_dir = ['J0','J1','J2','J3','J4','J5','J6','J7','J8','J9']



        imagetest_out_log = [ '识别结果： ' + sub_dir[int(test_result)]]



        # out1 = ''.join(imagetest_out_log)
        # a = random.randint(0, 360)
        # b = random.randint(0, 360) / 10
        #
        # c = random.randint(40, 80) * 50
        # d = random.randint(10, 30)
        # e = random.randint(10, 20)
        # f = random.randint(8, 20) * 10
        #
        #
        # self.textBrowserIMGProperty.setText(out1)
        # self.conventional_zhihxindu(sub_dir[int(test_result)])
        # self.textEditNoiseLocation.setText('方位{}度\n距离{}km'.format(a, b))
        # self.textEditNoiseParameter.setText('中心频率{}MHZ\n带宽{}MHZ\n干扰强度{}dB\n干扰功率{}W'.format(c, d, e, f))

    def controlBarCommand(self, command):
        if (command == 0):
            self.play()
            self.videoPlayerControlBar.enableButtons(True)

        elif (command == 1):
            self.pause()

        elif (command == 2):
            self.stop()
            self.videoPlayerControlBar.enableButtons(False)
            self.videoPlayerControlBar.enablePlayButton(True)

        elif (command == 3):
            # 10 seconds backward
            self.backward()
        # 10 frames backward
        # self.videoPlayer.backwardFrames()
        elif (command == 4):
            self.decreaseSpeed()
        elif (command == 5):
            self.increaseSpeed()
        elif (command == 6):
            # 10 seconds forward
            self.forward()
        # 10 frames forward
        # self.videoPlayer.forwardFrames()
        elif (command == 7):
            self.nextBreakpoint()

    def play(self):
        if (not self.timer.isActive()):
            self.timer.start()

    def pause(self):
        if (self.timer.isActive()):
            self.timer.stop()

    def stop(self):
        self.timer.stop()
        # Set videoCapture position
        self.videoCapture.set(cv2.CAP_PROP_POS_MSEC, 0)
        self.nextFrameSlot()

    def backward(self):
        # Get videoCapture position (in milliseconds)
        videoPos = self.videoCapture.get(cv2.CAP_PROP_POS_MSEC)
        # Move 10 seconds backward (if pos < 10000 milliseconds set pos to 0 milliseconds)
        if (videoPos < 10000):
            videoPos = 0
        else:
            videoPos -= 10000
        # Set videoCapture position
        self.videoCapture.set(cv2.CAP_PROP_POS_MSEC, videoPos)
        if (not self.timer.isActive()):
            self.nextFrameSlot()

    def backwardFrames(self):
        # Get videoCapture position (in frames)
        videoPos = self.videoCapture.get(cv2.CAP_PROP_POS_FRAMES)
        # Move 10 frames backward (if pos < 10 frames set pos to frame 0)
        if (videoPos < 10):
            videoPos = 0
        else:
            videoPos -= 10
        # Set videoCapture position
        self.videoCapture.set(cv2.CAP_PROP_POS_FRAMES, videoPos)
        if (not self.timer.isActive()):
            self.nextFrameSlot()

    def decreaseSpeed(self):
        if (self.speed < 4.0):
            self.speed *= 2.0
        self.timer.setInterval((1000. / self.videoCapture_fps) * self.speed)

    def increaseSpeed(self):
        if (self.speed > 0.25):
            self.speed /= 2.0
        self.timer.setInterval((1000. / self.videoCapture_fps) * self.speed)

    def forward(self):
        # Get videoCapture position (in milliseconds)
        videoPos = self.videoCapture.get(cv2.CAP_PROP_POS_MSEC)
        # Move 10 seconds forward (if pos+10s > video duration do nothing)
        if (videoPos < (self.duration - 10) * 1000):
            videoPos += 10000
        # Set videoCapture position
        self.videoCapture.set(cv2.CAP_PROP_POS_MSEC, videoPos)
        if (not self.timer.isActive()):
            self.nextFrameSlot()

    def forwardFrames(self):
        # Get videoCapture position (in frames)
        videoPos = self.videoCapture.get(cv2.CAP_PROP_POS_FRAMES)
        # Move 10 frames forward (if pos+10f > video nFrames do nothing)
        if (videoPos < (self.videoCapture_nFrame - 10)):
            videoPos += 10
        # Set videoCapture position
        self.videoCapture.set(cv2.CAP_PROP_POS_FRAMES, videoPos)
        if (not self.timer.isActive()):
            self.nextFrameSlot()

    def nextBreakpoint(self):
        (filename, _) = QFileDialog.getSaveFileName(None,'Save File', QDir.home().path(), "Image Files (*.jpg)")
        if filename:

            ret, frame = self.videoCapture.read()
            cv2.imwrite(filename, frame)

    def getDuration(self):
        return self.duration

    def getNumberFrameBySecond(self, second):
        return (second * self.videoCapture_fps)

    def getCurrentFrameNumber(self):
        return self.videoCapture.get(cv2.CAP_PROP_POS_FRAMES)

    def getCurrentSecond(self):
        return (self.videoCapture.get(cv2.CAP_PROP_POS_MSEC) / 1000)

    # def setting(self,cnn_choose,conventional_choose):
    #     if cnn_choose == True:
    #         self.CNN_setting = CNN_setting_widget()
    #         self.CNN_setting.show()
    #         # self.CNN_pichuli_model_dir = self.CNN_setting.lineEdit_CNN_classification_imagetest_model_address.text()
    #         # self.CNN_image_test_model_dir = self.CNN_setting.lineEdit_CNN_classification_imagetest_model_address.text()
    #         # CNN_setting.exec()
    #     if conventional_choose == True:
    #         self.if_conventional_setted = True
    #         self.conventional_setting = conventional_setting_widget()

            # self.conventional_setting.show()
    ##通过系统设置设定模型及数据
    def CNN_setting_all(self):
        self.CNN_setting = CNN_setting_widget()
        self.CNN_setting.show()

        if self.CNN_setting.radioButton_resnet18.isChecked() == True:
            self.model_name = 'resnet18'
        elif self.CNN_setting.radioButton_resnet50.isChecked() == True:
            self.model_name = 'resnet50'
        elif self.CNN_setting.radioButton_vgg16.isChecked() == True:
            self.model_name = 'vgg16'
            print(self.model_name)
        elif self.CNN_setting.radioButton_vgg19.isChecked() == True:
            self.model_name = 'vgg19'
            print(self.model_name)
        elif self.CNN_setting.radioButton_densenet121.isChecked() == True:
            self.model_name = 'densenet121'
            print(self.model_name)
        elif self.CNN_setting. radioButton_squeezenet.isChecked() == True:
            self.model_name = 'squeezenet'
            print(self.model_name)
        elif self.CNN_setting. radioButton_alexnet.isChecked() == True:
            self.model_name = 'alexnet'
            print(self.model_name)
        elif self.CNN_setting. radioButton_mobilenet.isChecked() == True:
            self.model_name = 'mobilenet'
            print(self.model_name)

        if self.CNN_setting.radioButton_at.isChecked() == True:
            self.defense_method_name = 'at'
            print(self.defense_method_name)
        elif self.CNN_setting.radioButton_trades.isChecked() == True:
            self.defense_method_name = 'trades'
            print(self.defense_method_name)
        elif self.CNN_setting.radioButton_dbyol.isChecked() == True:
            self.defense_method_name = 'dbyol'
            print(self.defense_method_name)
        elif self.CNN_setting.radioButton_nodefense.isChecked() == True:
            self.defense_method_name = 'nodefense'
            print(self.defense_method_name)

        if self.CNN_setting.radioButton_uc.isChecked() == True:
            self.data_name = 'uc'
            print(self.data_name)
        elif self.CNN_setting.radioButton_mstar.isChecked() == True:
            self.data_name = 'mstar'
            print(self.data_name)
        elif self.CNN_setting.radioButton_fusarship.isChecked() == True:
            self.data_name = 'fusarship'
            print(self.data_name)
        elif self.CNN_setting.radioButton_fgsc.isChecked() == True:
            self.data_name = 'fgsc'
            print(self.data_name)
        elif self.CNN_setting.radioButton_so2sat.isChecked() == True:
            self.data_name = 'so2sat'

        self.CNN_setting.pushButton.clicked.connect(self.CNN_setting.close)


    def choose_data_dir_pichuli_class(self):
        self.RS_dataset = RS_dataset_widget()
        self.RS_dataset.show()
        # self.dataset_setting=conventional_pichuli_dataset_setting_widget()
        # self.dataset_setting.show()
        # self.dataset_address = self.dataset_setting.lineEdit_dattaset_address.text()
        # print('ddddddddd',self.dataset_address)
        self.RS_dataset.pushButton.clicked.connect(self.pichuli_class_2)
        self.RS_dataset.pushButton.clicked.connect(self.RS_dataset.close)

    # def pichuli_class(self):
    #     self.videoPlayerControlBar.enableButtons(True)
    #     self.dataset_address = self.dataset_setting.lineEdit_dattaset_address.text()
    #     print(self.dataset_address)
    #     video_maker(file_name=self.dataset_address)
    #     self.video_filenames = 'pichuli/pichuli_data.mp4'
    #     self.setupVariables('pichuli/pichuli_data.mp4')

    def pichuli_class_2(self):
        self.videoPlayerControlBar.enableButtons(True)
        self.dataset_address = self.RS_dataset.lineEdit.text()
        print(self.dataset_address)
        video_maker(file_name=self.dataset_address)
        self.video_filenames = 'pichuli/pichuli_data.mp4'
        self.setupVariables('pichuli/pichuli_data.mp4')

    def video_rec(self):
        self.OPenVideo()

    def ImgRec(self):
        if self.if_image_opened == False:
            self.OPenIMG()


    def SampleDisplay(self):
        self.sample=LeftTabWidget()
        self.sample.setObjectName("典型干扰图像特性")
        self.sample.show()


    def image_class(self):
        if self.if_image_opened == False:
            self.OPenIMG()

        # import random
        # a = random.randint(0,360)
        # b = random.randint(0, 360) / 10
        # self.textEditNoiseLocation.setText('方位{}度\n距离{}km'.format(a,b))
        # self.textEditNoiseLocation.repaint()
        # c = random.randint(40,80)*50
        # d = random.randint(10,30)
        # e = random.randint(10, 20)
        # f = random.randint(8, 20)*10
        # self.textEditNoiseParameter.setText('中心频率{}MHZ\n带宽{}MHZ\n干扰强度{}dB\n干扰功率{}W'.format(c,d,e,f))
        #
        if self.radioButtonSVM.isChecked() == True:
            self.thread_image_test = Thread_CNN_image_adv_test(self.image_filename, self.CNN_image_test_model_dir, self.data_name)
            self.thread_image_test.update_imagetest.connect(self.get_CNN_image_test_result)
            self.thread_image_test.start()
            self.thread_image_test.exec()
        elif self.radioButtonCNN.isChecked() == True:
            print(self.image_filename, self.CNN_image_test_model_dir, self.data_name)
            self.thread_image_test = Thread_CNN_image_test(self.image_filename, self.CNN_image_test_model_dir, self.data_name)
            self.thread_image_test.update_imagetest.connect(self.get_CNN_image_test_result)
            self.thread_image_test.start()
            self.thread_image_test.exec()

    def get_CNN_image_test_result(self, image_test_result):
        self.textBrowserIMGProperty.setText(image_test_result[1])
        self.textBrowserIMGProperty.repaint()
        # print(image_test_result[2])
        self.CNN_zhixindu(image_test_result[2])
        engine = pyttsx3.init()
        engine.say('识别结果为')

        engine.say(image_test_result[4])
        engine.runAndWait()

    def get_conventional_image_test_result(self, image_test_result):
        self.textBrowserIMGProperty.setText(image_test_result[0])
        self.textBrowserIMGProperty.repaint()
        self.conventional_zhihxindu(image_test_result[2])

        engine = pyttsx3.init()
        engine.say('识别结果为')
        engine.say(image_test_result[2])
        engine.runAndWait()

    def get_conventional_pichuli_result(self, image_test_result):
        self.textBrowserIMGProperty.setText(image_test_result[0])
        self.textBrowserIMGProperty.repaint()
        jpg = QtGui.QPixmap(image_test_result[1]).scaled(self.IMGDisplay.width(), self.IMGDisplay.height())
        self.IMGDisplay.setPixmap(jpg)
        self.IMGDisplay.repaint()
        self.IMGDisplay.show()
        self.conventional_zhihxindu(image_test_result[2])
        # self.textEditNoiseLocation.setText('方位{}度\n距离{}km'.format(image_test_result[3][0], image_test_result[3][1]))
        # self.textEditNoiseParameter.setText('中心频率{}MHZ\n带宽{}MHZ\n干扰强度{}dB\n干扰功率{}W'.format(image_test_result[3][2], image_test_result[3][3], image_test_result[3][4], image_test_result[3][5]))

    def get_conventional_video_result(self, result):
        print(result[2])


    def get_CNN_pichuli_result(self, image_test_result):
        self.textBrowserIMGProperty.setText(image_test_result[1])
        self.textBrowserIMGProperty.repaint()
        jpg = QtGui.QPixmap(image_test_result[0]).scaled(self.IMGDisplay.width(), self.IMGDisplay.height())
        self.IMGDisplay.setPixmap(jpg)
        self.IMGDisplay.repaint()
        self.IMGDisplay.show()
        self.CNN_zhixindu(image_test_result[2])
        # self.textEditNoiseLocation.setText('方位{}度\n距离{}km'.format(image_test_result[5][0], image_test_result[5][1]))
        # self.textEditNoiseParameter.setText('中心频率{}MHZ\n带宽{}MHZ\n干扰强度{}dB\n干扰功率{}W'.format(image_test_result[5][2], image_test_result[5][3], image_test_result[5][4], image_test_result[5][5]))


    def stop_video(self):
        self.if_OPenCAM = 0
        self.if_OPenCAM_video_recording  = 0
        self.if_OPenCAM_imageset_recording = 0

        self.if_OPenVGA = 0
        self.if_OPenVGA_video_recording  = 0
        self.if_OPenVGA_imageset_recording = 0


    def clear(self):
        self.textBrowserFalseTarget.setText('')
        self.textBrowserNarrowNoise.setText('')
        self.textBrowserWideNoise.setText('')
        self.textBrowserScanNoise.setText('')
        self.textBrowserCombNoise.setText('')
        self.textBrowserOtherNoise.setText('')
        # self.textEditNoiseParameter.setText('')
        # self.textEditNoiseLocation.setText('')

    def conventional_zhihxindu(self,a):
        if a == 'J0':
            self.textBrowserFalseTarget.setText('0.00')
            self.textBrowserNarrowNoise.setText('0.00')
            self.textBrowserWideNoise.setText('0.00')
            self.textBrowserScanNoise.setText('0.00')
            self.textBrowserCombNoise.setText('0.00')
            self.textBrowserOtherNoise.setText('1.00')
        elif a == 'J1':
            self.textBrowserFalseTarget.setText('0.00')
            self.textBrowserNarrowNoise.setText('0.00')
            self.textBrowserWideNoise.setText('0.00')
            self.textBrowserScanNoise.setText('0.00')
            self.textBrowserCombNoise.setText('0.00')
            self.textBrowserOtherNoise.setText('1.00')
        elif a == 'J2':
            self.textBrowserFalseTarget.setText('0.00')
            self.textBrowserNarrowNoise.setText('0.00')
            self.textBrowserWideNoise.setText('0.00')
            self.textBrowserScanNoise.setText('1.00')
            self.textBrowserCombNoise.setText('0.00')
            self.textBrowserOtherNoise.setText('0.00')
        elif a == 'J3':
            self.textBrowserFalseTarget.setText('0.00')
            self.textBrowserNarrowNoise.setText('0.00')
            self.textBrowserWideNoise.setText('0.00')
            self.textBrowserScanNoise.setText('0.00')
            self.textBrowserCombNoise.setText('1.00')
            self.textBrowserOtherNoise.setText('0.00')
        elif a == 'J4':
            self.textBrowserFalseTarget.setText('0.00')
            self.textBrowserNarrowNoise.setText('0.00')
            self.textBrowserWideNoise.setText('0.00')
            self.textBrowserScanNoise.setText('0.00')
            self.textBrowserCombNoise.setText('1.00')
            self.textBrowserOtherNoise.setText('0.00')
        elif a == 'J5':
            self.textBrowserFalseTarget.setText('0.00')
            self.textBrowserNarrowNoise.setText('0.00')
            self.textBrowserWideNoise.setText('0.00')
            self.textBrowserScanNoise.setText('0.00')
            self.textBrowserCombNoise.setText('0.00')
            self.textBrowserOtherNoise.setText('1.00')
        else:
            self.textBrowserFalseTarget.setText('0.00')
            self.textBrowserNarrowNoise.setText('0.00')
            self.textBrowserWideNoise.setText('0.00')
            self.textBrowserScanNoise.setText('0.00')
            self.textBrowserCombNoise.setText('0.00')
            self.textBrowserOtherNoise.setText('0.00')
    def CNN_zhixindu(self,a):
        self.textBrowserFalseTarget.setText(str(float('%.2g' % a[6])))
        self.textBrowserNarrowNoise.setText(str(float('%.2g' % a[8])))
        self.textBrowserWideNoise.setText(str(float('%.2g' % a[9])))
        self.textBrowserScanNoise.setText(str(float('%.2g' % a[2])))
        self.textBrowserCombNoise.setText(str(float('%.2g' % a[3])))
        self.textBrowserOtherNoise.setText(str(float('%.2g' % a[5])))
    def retranslateUi(self, HQ16pATR):
        _translate = QtCore.QCoreApplication.translate

        HQ16pATR.setWindowTitle(_translate("HQ16pATR", "多模态遥感影像智能识别模型欺骗防御技术仿真验证软件系统"))
        HQ16pATR.setWindowIcon(QIcon("logo.ico"))


        self.groupBoxIMG.setTitle(_translate("HQ16pATR", "遥感图像"))
        self.groupBoxSET.setTitle(_translate("HQ16pATR", "工作模式设定"))
        self.pushButtonIMG.setText(_translate("HQ16pATR", "单幅影像识别"))
        self.pushButtonSEQ.setText(_translate("HQ16pATR", "对抗欺骗生成"))
        self.pushButtonVID.setText(_translate("HQ16pATR", "对抗欺骗检测"))
        self.pushButtonVGA.setText(_translate("HQ16pATR", "识别模型加固"))
        self.pushButtonMANUAL.setText(_translate("HQ16pATR", "模型鲁棒性评估"))
        self.radioButtonSVM.setText(_translate("HQ16pATR", "对抗欺骗"))
        self.radioButtonCNN.setText(_translate("HQ16pATR", "标准影像"))
#        self.radioButtonFUSION.setText(_translate("HQ16pATR", "融合识别"))
#        self.radioButtonMANUAL.setText(_translate("HQ16pATR", "人工判读"))
        self.toolButton.setText(_translate("HQ16pATR", "系统设置"))
        self.video_stop_Button.setText(_translate("HQ16pATR", "停止流"))
        self.textBrowserIMGProperty.setHtml(_translate("HQ16pATR", "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0//EN\" \"http://www.w3.org/TR/REC-html40/strict.dtd\">\n"
"<html><head><meta name=\"qrichtext\" content=\"1\" /><style type=\"text/css\">\n"
"p, li { white-space: pre-wrap; }\n"
"</style></head><body style=\" font-family:\'华文隶书\'; font-size:12pt; font-weight:400; font-style:normal;\">\n"
"<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><span style=\" font-size:10pt;\">雷达与图像属性</span></p></body></html>"))
#        self.radioButtonAUDIO.setText(_translate("HQ16pATR", "语音播放"))
        self.groupBoxRESULT.setTitle(_translate("HQ16pATR", "对抗欺骗评估指标"))
        self.labelFalseTarget.setText(_translate("HQ16pATR", "正确类别平均置信度"))
        self.textBrowserFalseTarget.setHtml(_translate("HQ16pATR", "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0//EN\" \"http://www.w3.org/TR/REC-html40/strict.dtd\">\n"
"<html><head><meta name=\"qrichtext\" content=\"1\" /><style type=\"text/css\">\n"
"p, li { white-space: pre-wrap; }\n"
"</style></head><body style=\" font-family:\'华文隶书\'; font-size:12pt; font-weight:400; font-style:normal;\">\n"
"<p style=\"-qt-paragraph-type:empty; margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><br /></p></body></html>"))
        self.labelNarrowNoise.setText(_translate("HQ16pATR", "对抗类别平均置信度"))
        self.labelWideNoise.setText(_translate("HQ16pATR", "平均结构相似性"))
        self.labelScan.setText(_translate("HQ16pATR", "扰动敏感距离"))
        self.labelComb.setText(_translate("HQ16pATR", "噪声容忍评估"))
#         self.textEditNoiseLocation.setHtml(_translate("HQ16pATR", "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0//EN\" \"http://www.w3.org/TR/REC-html40/strict.dtd\">\n"
# "<html><head><meta name=\"qrichtext\" content=\"1\" /><style type=\"text/css\">\n"
# "p, li { white-space: pre-wrap; }\n"
# "</style></head><body style=\" font-family:\'华文隶书\'; font-size:12pt; font-weight:400; font-style:normal;\">\n"
# "<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><span style=\" font-size:8pt; color:#ff0000;\">方位255度</span></p>\n"
# "<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><span style=\" font-size:8pt; color:#ff0000;\">距离1km</span></p></body></html>"))
#         self.textEditNoiseParameter.setHtml(_translate("HQ16pATR", "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0//EN\" \"http://www.w3.org/TR/REC-html40/strict.dtd\">\n"
# "<html><head><meta name=\"qrichtext\" content=\"1\" /><style type=\"text/css\">\n"
# "p, li { white-space: pre-wrap; }\n"
# "</style></head><body style=\" font-family:\'宋体\',\'华文隶书\'; font-size:8pt; font-weight:400; font-style:normal;\">\n"
# "<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><span style=\" font-family:\'华文隶书\'; color:#aaaa7f;\">中心频率3250MHZ</span></p>\n"
# "<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><span style=\" font-family:\'华文隶书\'; color:#aaaa7f;\">带宽20MHZ</span></p>\n"
# "<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><span style=\" font-family:\'华文隶书\'; color:#aaaa7f;\">干扰强度15dB</span></p>\n"
# "<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><span style=\" font-family:\'华文隶书\'; color:#aaaa7f;\">干扰功率100W</span></p></body></html>"))
        # self.labelNoiseLocation.setText(_translate("HQ16pATR", "干扰位置："))
        # self.labelNoiseParameter.setText(_translate("HQ16pATR", "干扰参数："))
        self.labelOtherNoise.setText(_translate("HQ16pATR", "神经元敏感度"))
        self.groupBoxADV.setTitle(_translate("HQ16pATR", "应用场景"))
        # self.startButton.setText(_translate("HQ16pATR", "滚动播放"))
        # self.previousButton.setText(_translate("HQ16pATR", "前一项"))
        # self.nextButton.setText(_translate("HQ16pATR", "后一项"))
        # self.stopButton.setText(_translate("HQ16pATR", "停止建议"))

##############gai主菜单
        # self.menuFile.setTitle(_translate("HQ16pATR", "遥感图像引接"))
        self.menuTrain.setTitle(_translate("HQ16pATR", "智能识别模型训练"))
        self.menuTest.setTitle(_translate("HQ16pATR", "对抗欺骗数据测试"))
        self.menuEvaluation.setTitle(_translate("HQ16pATR", "对抗欺骗影像生成"))
        self.menuManual.setTitle(_translate("HQ16pATR", "智能识别模型主动加固"))
        self.menuSettings.setTitle(_translate("HQ16pATR", "对抗欺骗被动防御"))

        self.actionshape.setText(_translate("HQ16pATR", "纹理特征"))
        self.actionIMGFile.setText(_translate("HQ16pATR", "单幅图像"))
        self.actionIMGFolder.setText(_translate("HQ16pATR", "图像序列"))

        self.actionCNNtrain.setText(_translate("HQ16pATR", "定制式深度智能学习"))
        self.actionCNNtrain_fugaishi.setText(_translate("HQ16pATR", "覆盖式深度智能学习"))
        #self.actionFUSIONtrain.setText(_translate("HQ16pATR", "融合识别"))
        #self.actionFUSIONtrain.setIconText(_translate("HQ16pATR", "融合识别"))
        self.action_2.setText(_translate("HQ16pATR", "几何特征"))
        self.action_6.setText(_translate("HQ16pATR", "深度编码"))
        self.action_7.setText(_translate("HQ16pATR", "时频特征"))
        self.actionstandardtest.setText(_translate("HQ16pATR", "遥感影像单张测试"))
        self.actionadvtest.setText(_translate("HQ16pATR", "遥感影像数据集批测试"))
        self.actionMultisource_adv_classification.setText(_translate("HQ16pATR", "多源对抗样本影像识别"))
        self.actionFusiontest.setText(_translate("HQ16pATR", "融合识别"))
        self.actionDNN.setText(_translate("HQ16pATR", "DNN分类"))
        self.actionadversarialattack.setText(_translate("HQ16pATR", "深度欺骗扰动"))
        self.actionblackattack.setText(_translate("HQ16pATR", "物理特性扰动"))
        self.actionsparseattack.setText(_translate("HQ16pATR", "稀疏扰动"))
        self.actionCAM.setText(_translate("HQ16pATR", "摄像头录屏采集"))
        #gai
        # self.actionCAMvideo_recording.setText(_translate("HQ16pATR", "摄像头录屏视频制作"))
        # self.actionCAMimageset_recording.setText(_translate("HQ16pATR", "摄像头录屏图像集制作"))
        self.actionVGA.setText(_translate("HQ16pATR", "VGA板卡采集"))
        # self.actionVGAvideo_recording.setText(_translate("HQ16pATR", "VGA录屏视频制作"))
        # self.actionVGAimageset_recording.setText(_translate("HQ16pATR", "VGA录屏图像集制作"))
        self.actionVIDEO.setText(_translate("HQ16pATR", "离线视频"))

        self.actiondefense_singlesource.setText(_translate("HQ16pATR", "单源特征强化加固"))
        self.actiondefense_valsource.setText(_translate("HQ16pATR", "主动加固欺骗识别指标验证"))
        self.actiondefense_multisource.setText(_translate("HQ16pATR", "多源特征融合加固"))
        self.actiondetector.setText(_translate("HQ16pATR", "深度特征检测器训练"))
        self.actionmodel.setText(_translate("HQ16pATR", "深度特征检测"))


    def caolianjie(self):
        self.actionVIDEO.triggered.connect(self.OPenVideo)
        self.actionVGA.triggered.connect(self.OPenVGA)
        # self.actionVGAvideo_recording.triggered.connect(self.OPenVGA_video_recording)
        # self.actionVGAimageset_recording.triggered.connect(self.OPenVGA_imageset_recording)
        self.actionCAM.triggered.connect(self.OPenCAM)
        # self.actionCAMvideo_recording.triggered.connect(self.OPenCAM_video_recording)
        # self.actionCAMimageset_recording.triggered.connect(self.OPenCAM_imageset_recording)
        # self.actionIMGFile.triggered.connect(self.OPenIMG)
        self.actionIMGFile.triggered.connect(self.clear)
        self.actionIMGFile.triggered.connect(self.image_class)
        self.actionIMGFolder.triggered.connect(self.OPenIMGFolder)
       # self.actiondefense_multisource.triggered.connect(self.OPenIMGforManual)
       #  self.actiondefense_singlesource.triggered.connect(self.SampleDisplay)
        self.actionmodel.triggered.connect(self.clear)
        self.actiondetector.triggered.connect(self.clear)

        self.if_image_opened = False
        self.if_conventional_setted = False
        self.CNN_pichuli_model_dir = 'pths\ResNet18.pth'

        self.model_name = "resnet18"
        self.data_name = "uc"
        self.CNN_image_test_model_dir = "pkl_save/{}_{}.pkl".format(self.data_name,self.model_name,self.data_name,
                                                                                self.model_name)

        self.pichuli_dataset_address = './Data/UCMerced_LandUse/val'

        # self.previousButton.clicked.connect(self.stackedWidget.slideInPrev)
        # self.nextButton.clicked.connect(self.stackedWidget.slideInNext)
        # self.startButton.clicked.connect(self.autoStart)
        # self.stopButton.clicked.connect(self.autoStop)

        self.toolButton.clicked.connect(self.CNN_setting_all)
        self.video_stop_Button.clicked.connect(self.stop_video)
        self.pushButtonIMG.clicked.connect(self.clear)
        self.pushButtonIMG.clicked.connect(self.image_class)

        # self.pushButtonSEQ.clicked.connect(self.choose_data_dir_pichuli_class)
        # self.pushButtonVID.clicked.connect(self.video_rec)
        # self.pushButtonVGA.clicked.connect(self.OPenVGA)
